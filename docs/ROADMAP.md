# Project Roadmap

This document tracks the evolution of dpx-showsite-ops from initial Govee monitoring to a full operations platform for show sites.

---

## ‚úÖ Phase 1: Core Data Pipeline (Completed)

**Goal**: Get Govee sensor data into a time-series database with visualization

**What we built:**
- Docker Compose stack with 5 services
- govee2mqtt polling Govee Cloud API every 10 minutes
- MQTT broker for pub/sub messaging
- Telegraf for MQTT‚ÜíInfluxDB routing
- InfluxDB 2.x for time-series storage
- Grafana for dashboards

**Status**: ‚úÖ Complete - data flowing end-to-end

---

## ‚úÖ Phase 2: External Access & Network (Completed)

**Goal**: Make the stack accessible remotely and fix network issues

**What we built:**
- Static IP configuration (<server-ip>)
- mDNS support via avahi-daemon (dpx-showsite-ops.local)
- Tailscale mesh VPN for secure remote SSH
- Cloudflare Tunnel for temporary public dashboard sharing
- Grafana public dashboard links

**Status**: ‚úÖ Complete - accessible from anywhere

---

## ‚úÖ Phase 2.5: Friendly Name Tags (Completed)

**Goal**: Replace MAC addresses with human-readable device names in data

**What we built:**
- Telegraf regex processors to extract device_id from MQTT topics
- Telegraf enum processors to map device_id ‚Üí device_name and room
- `update-device-map.sh` script to fetch device info from govee2mqtt API
- Hourly cron job to auto-update mappings when devices change

**Status**: ‚úÖ Complete - tags confirmed working in InfluxDB and Grafana

---

## ‚úÖ Phase 2.6: IPv6 Network Fix (Completed)

**Goal**: Fix govee2mqtt AWS IoT connection timeouts

**What we fixed:**
- Identified IPv6 causing timeouts to AWS IoT endpoint
- Disabled IPv6 on eth0 at kernel level
- Made permanent via /etc/sysctl.conf
- Documented in troubleshooting guide

**Status**: ‚úÖ Complete - govee2mqtt connects reliably

---

## ‚úÖ Phase 2.7: Hostname Cleanup (Completed)

**Goal**: Use RFC-compliant hostname and rebrand for reusability

**What we did:**
- Renamed server from `dpx-coachella-ops` to `dpx-showsite-ops`
- Updated /etc/hosts and avahi-daemon
- Verified mDNS and Tailscale auto-updated
- Established naming convention for multi-site deployments

**Status**: ‚úÖ Complete - hostname clean and discoverable

---

## üöß Phase 3: Deployment & Documentation (In Progress)

**Goal**: Make the stack reproducible and shareable

**What we're building:**

### Phase 3.1: Deployment Automation
- [x] Create `setup.sh` for initial deployment
- [x] Create `.env.example` template with comments
- [x] Enhance `manage.sh` with better help and error handling
- [x] Fix `iot` wrapper script (not symlink) to handle paths correctly
- [x] Create `backup-restore.sh` script for volume backups
- [x] Test full deployment from scratch

### Phase 3.2: Documentation
- [x] Write README.md with quick start guide
- [x] Write ROADMAP.md (this file)
- [x] Write ARCHITECTURE.md with technical deep dive
- [x] Create CHANGELOG.md for version history
- [x] Document all known issues and fixes
- [x] Create service-specific docs (Govee quirks, etc.)

### Phase 3.3: Version Control
- [x] Create .gitignore
- [x] Initialize Git repository
- [x] Tag v1.0 release
- [x] Push to GitHub as `dpx-showsite-ops`
- [x] Set up comprehensive documentation links

**Status**: ‚úÖ Complete - all deliverables shipped

---

## ÔøΩ Phase 4: BLE Gateway Integration (In Progress)

**Goal**: Read sensors locally via BLE instead of waiting for cloud sync

**What we've built:**

### Phase 4.1: BLE Gateway (Parallel Paths) ‚úÖ
- ‚úÖ ESP32 gateway deployed (192.168.1.213, OpenMQTTGateway v1.8.1)
- ‚úÖ Theengs Gateway deployed (Windows NUC, 192.168.1.68)
- ‚úÖ Both publish to standardized MQTT topics
- Both gateways operational with redundancy

### Phase 4.2: BLE Decoder Service ‚è≥
- ‚úÖ ble_decoder.py script created and functional
- ‚úÖ Subscribes to both ESP32 and Theengs MQTT topics
- ‚úÖ Decodes H5051/H507x manufacturerdata
- ‚úÖ Publishes to demo_showsite topics with source tagging
- ‚úÖ Loads device mappings from govee2mqtt API
- ‚è≥ **Pending**: Dockerization (currently runs manually)

### Phase 4.3: Unified Telegraf Config ‚úÖ
- ‚úÖ Modular telegraf.conf structure (base config + device-mappings.conf)
- ‚úÖ Two MQTT inputs configured:
  - Cloud source: gv2mqtt/# topics (10min intervals)
  - BLE source: demo_showsite/# topics (real-time)
- ‚úÖ Source tagging implemented (gv_cloud, dpx_ops_decoder)
- ‚úÖ BLE regex processor extracts source_node, room, device_name, sensor_type
- ‚úÖ Dynamic enum mappings regenerated by update-device-map.sh

### Phase 4.4: Grafana Dashboard Update ‚úÖ
- ‚úÖ Dashboards created with time series, gauges, stat panels
- ‚úÖ Queries handle multiple sources (gv_cloud, dpx_ops_decoder)
- ‚úÖ Custom display names via map() showing source, node, room, device
- ‚úÖ Real-time data flowing from both cloud and BLE sources

**Benefits:**
- Sub-second latency instead of 10-20 minutes
- No dependency on Govee cloud uptime
- More frequent readings (every BLE broadcast)
- Better debugging (see exactly what sensor transmits)

### Outstanding Items

#### Critical
1. **MQTT Retained Message Ghost Data** - ble_decoder.py publishes with `retain=True` to topics containing device names. When devices are renamed (e.g., `studio_5051_down` ‚Üí `5051_studio_down`), old retained messages persist on old topic paths. Every Telegraf restart (hourly cron) causes resubscription, replaying both old and new retained messages, creating duplicate time series in InfluxDB with frozen ghost data.
   - **Trigger**: update-device-map.sh unconditionally restarts Telegraf hourly (no diff check)
   - **Impact**: Stale data appears in Grafana dashboards alongside current data
   - **Workaround**: Manually clear old retained messages with `mosquitto_pub -r -n`
   - **Fix plan**: See [plan-mqtt-cleanup.md](context_public/plan-mqtt-cleanup.md)

2. **update-device-map.sh Lacks Diff Check** - Script unconditionally restarts Telegraf every hour even when device mappings haven't changed, causing unnecessary service interruptions and replaying retained messages.
   - **Current**: Always runs `docker compose restart telegraf`
   - **Needed**: Compare new config to existing, only restart if different
   - **Benefit**: Reduces hourly disruptions, minimizes ghost data exposure

3. **BLE Decoder restart on name changes** - Decoder loads device map once at startup; needs API polling or cron restart

#### High Priority
4. **Dockerize ble_decoder.py** - Create Dockerfile, add to docker-compose.yml, auto-start with stack
5. **ESP32 pubadvdata persistence** - Setting resets on reboot; need auto-enable script or systemd timer

#### Medium Priority
6. **Telegraf "Available" error suppression** - govee2mqtt status messages trigger parse errors (low impact)
7. **MAC-based device filtering** - Use z_device_id instead of device_name for stability across renames
8. **Theengs Gateway auto-start** - Windows scheduled task or NSSM service needed

**Status**: üîÑ In Progress - core functionality working, containerization and cleanup pending

---

## üìã Phase 5: Network Device Backups (Planned)

**Goal**: Automated backups of network infrastructure configs

**What we'll build:**

### Phase 5.1: TFTP Server
- Add TFTP service to docker-compose.yml
- Configure storage volume for switch configs
- Set up access controls (read/write paths)

### Phase 5.2: M4300 Backup Scripts
- Pull/integrate existing m4300-scripts repository
- Automate config downloads via TFTP
- Schedule daily backups via cron
- Git-commit configs for version history
- Alert on backup failures

### Phase 5.3: Monitoring Integration
- Track backup job success/failure in InfluxDB
- Grafana dashboard for backup status
- Slack/email alerts on failures (optional)

**Status**: üìã Planned - existing scripts need integration

---

## ÔøΩ Phase 6: Set Schedule Integration (Planned)

**Goal**: Integrate real-time show schedule tracking for festival operations

**Sean's Repo**: https://github.com/macswg/coachella_set_schedule

**What we'll build:**

### Phase 6.1: Git Submodule Setup
- Add Sean's repo as `services/set-schedule/` submodule
- Enables easy updates without code duplication
- Tracks his commits independently

### Phase 6.2: Docker Service
- Create `Dockerfile.showsite` for containerization
- Add `set-schedule` service to docker-compose.yml
- Runs on port 8000 alongside other services
- Restart policy and volume mounts configured

### Phase 6.3: CLI Integration
- Add `iot ls` command for set-schedule logs
- Add `iot web` URL for schedule access
- Update `manage.sh` to handle set-schedule lifecycle

### Phase 6.4: Update Scripts
- Update `setup.sh` for submodule initialization
- Update `manage.sh` for logs and web commands

**What It Is:**
- FastAPI web app for real-time schedule tracking
- WebSocket sync across clients
- Google Sheets integration
- Operator mode (edit times) + view-only mode
- Tracks "slip" (lateness vs scheduled times)

**Benefits:**
- Central operations dashboard for set times
- Real-time visibility across mobile + desktop clients
- Historical tracking for post-event analysis
- Optional: Log slip data to InfluxDB for Grafana dashboards

**Status**: üìã Planned - can be done anytime (independent of 4/5)

---

## üîÆ Future Phases (Ideas)

### Phase 7: Additional Sensor Types
- Motion sensors (PIR)
- Light sensors
- Door/window sensors
- Energy monitoring (smart plugs)

### Phase 8: Automation & Control
- Home Assistant integration
- Scene triggers based on sensor data
- HVAC control based on temperature readings
- Lighting automation

### Phase 9: Alert System
- Slack/Discord/email notifications
- Temperature threshold alerts
- Device offline detection
- API health monitoring

### Phase 10: Multi-Site Support
- Replicate stack to additional show sites
- Centralized monitoring dashboard
- Site comparison views
- Federated data queries

---

## Success Metrics

**Phase 3 (Complete):**
- [x] Someone can deploy from scratch in under 15 minutes
- [x] Zero manual config file editing after setup.sh
- [x] All secrets in .env, nothing hardcoded
- [x] Documentation covers 90%+ of common issues

**Phase 4:**
- [ ] BLE latency under 5 seconds
- [ ] Zero data loss during cloud outages
- [ ] Both sources visible in Grafana with source tag
- [ ] Windows NUC Theengs Gateway configured and tested

**Phase 5:**
- [ ] Network configs backed up daily
- [ ] 30-day retention of config history
- [ ] Recovery time < 5 minutes for switch restore

**Phase 6:**
- [ ] Set-schedule service running in Docker
- [ ] Real-time schedule updates via WebSocket
- [ ] Manual set time tracking working end-to-end
- [ ] Optional: Slip data flowing to InfluxDB

---

## Timeline

**Phase 3**: ‚úÖ Complete (2026-02-05)  
**Phase 4**: Ready to start (2026-02-05) - use Theengs Gateway on Windows NUC  
**Phase 5**: After Phase 4 completes - hardware available at studio  
**Phase 6**: Can start anytime (independent of 4/5) - Sean's repo ready  

**Custom ESP32 hardware**: Optional future enhancement, not blocking current phases

---

## Notes

- Each phase builds on previous phases without breaking existing functionality
- Phases can be skipped or reordered based on priority
- Phase 4 will use existing Windows NUC with Theengs Gateway (already tested)
- Custom ESP32 BLE gateway is a nice-to-have for later, not required
- Phase 5 depends on stabilizing M4300 scripts in separate repo
- Phase 6 is independent and can run in parallel with 4/5
- Naming convention (dpx-showsite-ops) enables multi-site deployments
